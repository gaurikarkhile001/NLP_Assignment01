{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slzVgaHsAvrk",
        "outputId": "5753bda6-75c4-485a-ddeb-1246e3847e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python NLTK** | WhitespaceTokenizer"
      ],
      "metadata": {
        "id": "u2oGovRRCuNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the help of *nltk.tokenize.WhitespaceTokenizer()* method, we are able to extract the tokens from string of words or sentences without whitespaces, new line and tabs by using *tokenize.WhitespaceTokenizer()* method.\n",
        "\n"
      ],
      "metadata": {
        "id": "OvsueXG4C3HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import WhitespaceTokenizer() method from nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "wt = WhitespaceTokenizer()\n",
        "\n",
        "# Create a string input\n",
        "text = \"my name is gauri ramesh karkhile. I'm third year computer science and engineering student at VIIT Pune.\"\n",
        "\n",
        "# Use tokenize method\n",
        "output = wt.tokenize(text)\n",
        "\n",
        "print(\"original text: \" + text)\n",
        "\n",
        "print(\"split the text using whitespace\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdt3H3nGA2_5",
        "outputId": "ea37b5c6-baef-40d8-c583-b1a4e9b2f6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: my name is gauri ramesh karkhile. I'm third year computer science and engineering student at VIIT Pune.\n",
            "split the text using whitespace\n",
            "['my', 'name', 'is', 'gauri', 'ramesh', 'karkhile.', \"I'm\", 'third', 'year', 'computer', 'science', 'and', 'engineering', 'student', 'at', 'VIIT', 'Pune.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python NLTK** | WordPunctTokenizer"
      ],
      "metadata": {
        "id": "AuAPwtdRE1Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WordPunctTokenizer in NLTK splits text into words and punctuation marks, treating punctuation as separate tokens. It's useful for basic tokenization tasks where punctuation carries meaning."
      ],
      "metadata": {
        "id": "ufUBMEQLTAJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# Create a string input\n",
        "text = \"my name is gauri ramesh karkhile. I'm third year computer science and engineering student at VIIT Pune.\"\n",
        "text3 = \"Hello user! Check out AI advancements. 😀🤬 #EMOJI 123\"\n",
        "# Use tokenize method\n",
        "output = WordPunctTokenizer().tokenize(text3)\n",
        "print(\"original text: \" + text3)\n",
        "\n",
        "print(\"split the text using WordPunctTokenizer\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJvazJz1ENr5",
        "outputId": "8bea2126-17b3-4f8a-eda3-036ac1bc6ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: Hello user! Check out AI advancements. 😀🤬 #EMOJI 123\n",
            "split the text using WordPunctTokenizer\n",
            "['Hello', 'user', '!', 'Check', 'out', 'AI', 'advancements', '.', '😀🤬', '#', 'EMOJI', '123']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python NLTK** | TreebankTokenzier"
      ],
      "metadata": {
        "id": "Y6h8vuyiFlBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The TreebankWordTokenizer is a tokenizer in NLTK that is trained on the Penn Treebank corpus. It is a more sophisticated tokenizer than the WordPunctTokenizer and can handle a wider range of punctuation and contractions."
      ],
      "metadata": {
        "id": "9lBW__quTIq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# Create a string input\n",
        "text = \"my name is gauri ramesh karkhile. I'm third year computer science and engineering student at VIIT Pune.\"\n",
        "\n",
        "# Use tokenize method\n",
        "\n",
        "output =  TreebankWordTokenizer().tokenize(text)\n",
        "print(\"original text: \" + text)\n",
        "\n",
        "print(\"split the text using TreebankTokenzier\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HkBme4GFdgZ",
        "outputId": "9fab3d8f-50cc-4b17-9ca8-bcf5a08ca5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: my name is gauri ramesh karkhile. I'm third year computer science and engineering student at VIIT Pune.\n",
            "split the text using TreebankTokenzier\n",
            "['my', 'name', 'is', 'gauri', 'ramesh', 'karkhile.', 'I', \"'m\", 'third', 'year', 'computer', 'science', 'and', 'engineering', 'student', 'at', 'VIIT', 'Pune', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python nltk** | Tweet Tokenizer"
      ],
      "metadata": {
        "id": "98V2hIV7H1Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TweetTokenizer is specifically designed for tokenizing tweets. It handles common Twitter conventions like hashtags, mentions, and emoticons."
      ],
      "metadata": {
        "id": "engXiooJTWQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "# Create a string input\n",
        "text3 = \"Hello user! Check out AI advancements. 1 3 5 😀🤬 #EMOJI\"\n",
        "\n",
        "# Use tokenize method\n",
        "output =  TweetTokenizer().tokenize(text3)\n",
        "print(\"original text: \" + text3)\n",
        "\n",
        "print(\"split the text using Tweet Tokenizer\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kegAgDv6Fzd5",
        "outputId": "ef283812-ee2b-4b80-d194-ead97f0573e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: Hello user! Check out AI advancements. 1 3 5 😀🤬 #EMOJI\n",
            "split the text using Tweet Tokenizer\n",
            "['Hello', 'user', '!', 'Check', 'out', 'AI', 'advancements', '.', '1', '3', '5', '😀', '🤬', '#EMOJI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python nltk** | Multi-word Expression Tokeziner"
      ],
      "metadata": {
        "id": "3RtwZZF6NrvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MWETokenizer is used to tokenize multi-word expressions (MWEs) as single units. MWEs are phrases that have a specific meaning when used together, such as \"kick the bucket,\" \"out of the blue,\" or \"by and large\"."
      ],
      "metadata": {
        "id": "P8K1IxluTbys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "# Create a string input\n",
        "text3 = \"Hello user! Check out AI advancements. 1 3 5 😀🤬 #EMOJI\"\n",
        "\n",
        "# Use tokenize method\n",
        "output =  MWETokenizer().tokenize(text3)\n",
        "print(\"original text: \" + text3)\n",
        "\n",
        "print(\"split the text using Multi-word Expression Tokeziner\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq5NScc6N0ti",
        "outputId": "5f9067a3-9a15-445c-a2fa-40a6e637f703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: Hello user! Check out AI advancements. 1 3 5 😀🤬 #EMOJI\n",
            "split the text using Multi-word Expression Tokeziner\n",
            "['H', 'e', 'l', 'l', 'o', ' ', 'u', 's', 'e', 'r', '!', ' ', 'C', 'h', 'e', 'c', 'k', ' ', 'o', 'u', 't', ' ', 'A', 'I', ' ', 'a', 'd', 'v', 'a', 'n', 'c', 'e', 'm', 'e', 'n', 't', 's', '.', ' ', '1', ' ', '3', ' ', '5', ' ', '😀', '🤬', ' ', '#', 'E', 'M', 'O', 'J', 'I']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "tokeziner = MWETokenizer([('a', 'lot'), ('a', 'little')])\n",
        "# Pass the MWE as a single list or tuple:\n",
        "tokeziner.add_mwe(('name','is','gauri'))\n",
        "\n",
        "#Assuming 'tokenizer' refers to 'tokeziner'\n",
        "print(tokeziner.tokenize('my name is gauri ramesh karkhile. I have a lot of work'.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59jz5-hPN_PD",
        "outputId": "0f96301a-9dc2-4b44-ee20-50a4a6cb9a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'name_is_gauri', 'ramesh', 'karkhile.', 'I', 'have', 'a_lot', 'of', 'work']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "tokeziner = MWETokenizer([('1', '3'), ( '😀','🤬')])\n",
        "# Pass the MWE as a single list or tuple:\n",
        "tokeziner.add_mwe(('hello','user'))\n",
        "\n",
        "#Assuming 'tokenizer' refers to 'tokeziner'\n",
        "print(tokeziner.tokenize('Hello user! Check out AI advancements. 1 3 5 😀🤬 #EMOJI'.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN4yTO5YV-W7",
        "outputId": "33711717-faec-4808-d4c6-85d1dac6e6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'user!', 'Check', 'out', 'AI', 'advancements.', '1_3', '5', '😀🤬', '#EMOJI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "8UUIt-06I_vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter’s Stemmer**\n",
        "It uses a set of heuristic rules to iteratively remove suffixes. Example: EED -> EE means “if the word has at least one vowel and consonant plus EED ending, change the ending to EE” as ‘agreed’ becomes ‘agree’."
      ],
      "metadata": {
        "id": "unkd_iqeJK4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vVHzNvM1Jpt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer"
      ],
      "metadata": {
        "id": "KKoUB_UBJMHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = LancasterStemmer()\n",
        "r = RegexpStemmer(\"ing\")\n",
        "p = PorterStemmer()\n",
        "s = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "IkNPaueCJhQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s.stem(\"playing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "v_U_Cp3IJrfk",
        "outputId": "0f625fa4-cfd9-47c5-fae3-6454a0b335a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'play'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "SnowballStemmer.languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G367kwOfJ9h3",
        "outputId": "8cb426c8-be6c-4394-c599-c4782c12e171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "words= [\"running\", \"jumps\", \"happily\", \"happilly\", \"excitment\"]\n",
        "\n",
        "stemmed_words = [porter_stemmer.stem(words) for words in words]\n",
        "\n",
        "print(\"original words:\", words)\n",
        "print(\"stemmed words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GhI5czoKF8Z",
        "outputId": "94711f2a-5e26-478f-a630-fbbf56e5eff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original words: ['running', 'jumps', 'happily', 'happilly', 'excitment']\n",
            "stemmed words: ['run', 'jump', 'happili', 'happilli', 'excit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "r68AYIX9K-x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_words(text):\n",
        "  return ' '.join([ps.stem(word) for word in text.split()])\n",
        "\n",
        "  sample= 'walk walks walking walked'\n",
        "  stem_words(sample)"
      ],
      "metadata": {
        "id": "gQBMNQWpLK9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemmer**\n",
        "\n",
        "An extension of the Porter Stemmer with more robust rules. The Snowball Stemmer, compared to the Porter Stemmer, is multi-lingual as it can handle non-English words. It supports various languages and is based on the ‘Snowball’ programming language, known for efficient processing of small strings."
      ],
      "metadata": {
        "id": "c2d6sMYhL8fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "words_to_stem = ['running', 'jumps', 'happily', 'happilly']\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
        "\n",
        "print(\"original words:\", words_to_stem)\n",
        "print(\"stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTkWniuyL-0Y",
        "outputId": "e4e6b830-c902-4260-c713-82cb8f1b55f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original words: ['running', 'jumps', 'happily', 'happilly']\n",
            "stemmed words: ['run', 'jump', 'happili', 'happilli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer**\n",
        "\n",
        "The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. But they are not as efficient as Snowball Stemmers. The Lancaster stemmers save the rules externally and basically uses an iterative algorithm."
      ],
      "metadata": {
        "id": "206k2hQNMVH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "words_to_stem = ['running', 'jumps', 'happily', 'happilly']\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
        "\n",
        "print(\"original words:\", words_to_stem)\n",
        "print(\"stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA7PvoTuMXlO",
        "outputId": "11931825-b668-4e69-b4a1-12d6f7382146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original words: ['running', 'jumps', 'happily', 'happilly']\n",
            "stemmed words: ['run', 'jump', 'happy', 'happil']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer**"
      ],
      "metadata": {
        "id": "_oiAfuoKMmu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVEEwfTSQL-I",
        "outputId": "7b026b31-3493-425e-832c-5bd24819a9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "\n",
        "sentence = \"Love looks not with the eyes but with the mind, and therefore is winged Cupid painted blind.\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "id": "Ch1Wq0dMMqaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5601abd2-8c98-429a-9689-543ddb1e3b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['Love', 'looks', 'not', 'with', 'the', 'eyes', 'but', 'with', 'the', 'mind', ',', 'and', 'therefore', 'is', 'winged', 'Cupid', 'painted', 'blind', '.']\n",
            "Stemmed words: ['lov', 'look', 'not', 'with', 'the', 'ey', 'but', 'with', 'the', 'mind', ',', 'and', 'theref', 'is', 'wing', 'cupid', 'paint', 'blind', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regexp Stemmer**\n",
        "\n",
        "The Regexp Stemmer, or Regular Expression Stemmer, is a stemming algorithm that utilizes regular expressions to identify and remove suffixes from words. It allows users to define custom rules for stemming by specifying patterns to match and remove."
      ],
      "metadata": {
        "id": "EBP44aFBQQs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "words = ['running', 'jumps', 'happily', 'happilly']\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmGykTVQQR-m",
        "outputId": "a882e269-2c18-4b4a-fc0d-e51c378a7db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'jumps', 'happily', 'happilly']\n",
            "Stemmed words: ['runn', 'jump', 'happily', 'happilly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**\n",
        "\n",
        "Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma. This technique considers the context and the meaning of the words, ensuring that the base form belongs to the language's dictionary. For example, the words \"running,\" \"ran,\" and \"runs\" are all lemmatized to the lemma \"run.\""
      ],
      "metadata": {
        "id": "Ye2ltSWMQnK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVrNJNd4Qom1",
        "outputId": "a4bc4bd0-aa68-4d3f-a5d3-41a3236c3250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wl = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "bGb6C1TrQyMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wl.lemmatize(\"mice\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3HeUW9mVQ2RM",
        "outputId": "c3f5d53e-4d98-48d7-9673-ed14f39d91fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mouse'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-jym4rOQ_kW",
        "outputId": "c885b744-4592-4c9b-9859-91bdefce93f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', 'driving', 'died', 'tried', 'feet']\n",
        "for words in list1:\n",
        "    print(words + \" ---> \" + wnl.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQEjWhbQRRRG",
        "outputId": "d0231c5b-2cdc-4371-ca0f-3e44bc5593b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kites ---> kite\n",
            "babies ---> baby\n",
            "dogs ---> dog\n",
            "flying ---> flying\n",
            "smiling ---> smiling\n",
            "driving ---> driving\n",
            "died ---> died\n",
            "tried ---> tried\n",
            "feet ---> foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence lemmatization examples**"
      ],
      "metadata": {
        "id": "VUDmAND9RwFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# Converting String into tokens\n",
        "list2 = nltk.word_tokenize(string)\n",
        "print(list2)\n",
        "#> ['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on',\n",
        "# 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
        "\n",
        "lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])\n",
        "\n",
        "print(lemmatized_string)\n",
        "#> the cat is sitting with the bat on the striped mat under many flying goose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyoYXNk5Rgae",
        "outputId": "31c0b48e-122b-44ec-b569-a5ee9d1e5547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on', 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
            "the cat is sitting with the bat on the striped mat under many flying goose\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After stopwords removal**"
      ],
      "metadata": {
        "id": "MnKXlQGjS9Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aC_a91tS-jh",
        "outputId": "095042d1-c151-4afe-ca65-ffda49cb2db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "for words in list2:\n",
        "  if words not in stopwords.words('english'):\n",
        "    print(words + \" ---> \" + wnl.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlSLnO3KTBo_",
        "outputId": "9c50fbd8-7cf1-4903-94e8-d1373d10db3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat ---> cat\n",
            "sitting ---> sitting\n",
            "bats ---> bat\n",
            "striped ---> striped\n",
            "mat ---> mat\n",
            "many ---> many\n",
            "flying ---> flying\n",
            "geese ---> goose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wordnet Lemmatizer (with POS tag)**\n",
        "\n",
        "Words like ‘sitting’, ‘flying’ etc remained the same after lemmatization. This is because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of Speech) tags."
      ],
      "metadata": {
        "id": "4I4bFS0kTQpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define function to lemmatize each word with its POS tag\n",
        "\n",
        "# POS_TAGGER_FUNCTION : TYPE 1\n",
        "def pos_tagger(nltk_tag):\n",
        "\tif nltk_tag.startswith('J'):\n",
        "\t\treturn wordnet.ADJ\n",
        "\telif nltk_tag.startswith('V'):\n",
        "\t\treturn wordnet.VERB\n",
        "\telif nltk_tag.startswith('N'):\n",
        "\t\treturn wordnet.NOUN\n",
        "\telif nltk_tag.startswith('R'):\n",
        "\t\treturn wordnet.ADV\n",
        "\telse:\n",
        "\t\treturn None\n",
        "\n",
        "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
        "\n",
        "# tokenize the sentence and find the POS tag for each token\n",
        "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "\n",
        "print(pos_tagged)\n",
        "#>[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'),\n",
        "# ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'),\n",
        "# ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('flying', 'VBG'), ('geese', 'JJ')]\n",
        "\n",
        "# As you may have noticed, the above pos tags are a little confusing.\n",
        "\n",
        "# we use our own pos_tagger function to make things simpler to understand.\n",
        "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
        "print(wordnet_tagged)\n",
        "#>[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None),\n",
        "# ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'),\n",
        "# ('mat', 'n'), ('under', None), ('many', 'a'), ('flying', 'v'), ('geese', 'a')]\n",
        "\n",
        "lemmatized_sentence = []\n",
        "for word, tag in wordnet_tagged:\n",
        "\tif tag is None:\n",
        "\t\t# if there is no available tag, append the token as is\n",
        "\t\tlemmatized_sentence.append(word)\n",
        "\telse:\n",
        "\t\t# else use the tag to lemmatize the token\n",
        "\t\tlemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
        "\n",
        "print(lemmatized_sentence)\n",
        "#> the cat can be sit with the bat on the striped mat under many fly geese\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU7g4YWFTovn",
        "outputId": "b8220a64-e108-4659-c844-7b8cecac1a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('badly', 'RB'), ('flying', 'VBG'), ('geese', 'JJ')]\n",
            "[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), ('mat', 'n'), ('under', None), ('many', 'a'), ('badly', 'r'), ('flying', 'v'), ('geese', 'a')]\n",
            "the cat be sit with the bat on the striped mat under many badly fly geese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TextBlob**\n",
        "\n",
        "TextBlob is a python library used for processing textual data. It provides a simple API to access its methods and perform basic NLP tasks.\n",
        "\n",
        "Download TextBlob package : In your anaconda prompt or terminal, type: pip install textblob"
      ],
      "metadata": {
        "id": "5RZ3fsNiTuhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word\n",
        "\n",
        "my_word = 'cats'\n",
        "\n",
        "# create a Word object\n",
        "w = Word(my_word)\n",
        "\n",
        "print(w.lemmatize())\n",
        "#> cat\n",
        "\n",
        "sentence = 'the bats saw the cats with stripes hanging upside down by their feet.'\n",
        "\n",
        "s = TextBlob(sentence)\n",
        "lemmatized_sentence = \" \".join([w.lemmatize() for w in s.words])\n",
        "\n",
        "print(lemmatized_sentence)\n",
        "#> the bat saw the cat with stripe hanging upside down by their foot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZVFqVj4Tv9O",
        "outputId": "bb8d6eae-b8ed-48cb-b734-0fe61a8c5d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "the bat saw the cat with stripe hanging upside down by their foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TextBlob (with POS tag)**\n",
        "\n",
        "Same as in Wordnet approach without using appropriate POS tags, we observe the same limitations in this approach as well. So, we use one of the more powerful aspects of the TextBlob module the ‘Part of Speech’ tagging to overcome this problem."
      ],
      "metadata": {
        "id": "-a54z7PdT0e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Define function to lemmatize each word with its POS tag\n",
        "\n",
        "# POS_TAGGER_FUNCTION : TYPE 2\n",
        "def pos_tagger(sentence):\n",
        "\tsent = TextBlob(sentence)\n",
        "\ttag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
        "\twords_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
        "\tlemma_list = [wd.lemmatize(tag) for wd, tag in words_tags]\n",
        "\treturn lemma_list\n",
        "\n",
        "# Lemmatize\n",
        "sentence = \"the bats saw the cats with stripes hanging upside down by their feet\"\n",
        "lemma_list = pos_tagger(sentence)\n",
        "lemmatized_sentence = \" \".join(lemma_list)\n",
        "print(lemmatized_sentence)\n",
        "#> the bat saw the cat with stripe hang upside down by their foot\n",
        "t_blob = TextBlob(sentence)\n",
        "lemmatized_sentence = \" \".join([w.lemmatize() for w in t_blob.words])\n",
        "print(lemmatized_sentence)\n",
        "#> the bat saw the cat with stripe hanging upside down by their foot\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-4xfBcUT13e",
        "outputId": "904d1e78-1a18-405a-c8c5-f3a13a0f7992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bat saw the cat with stripe hang upside down by their foot\n",
            "the bat saw the cat with stripe hanging upside down by their foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing with Tokenization and Lemmatization**"
      ],
      "metadata": {
        "id": "zzHppXZpT-S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBel4ApST7iG",
        "outputId": "33c64d88-2b4c-40be-cffe-eb5abee5d3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sample text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Function to get the part of speech tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "\n",
        "# Print results\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Tokenized Words: \", words)\n",
        "print(\"Stemmed Words: \", stemmed_words)\n",
        "print(\"Lemmatized Words: \", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_d3YiI7UB1v",
        "outputId": "bcc28b86-782a-4953-91a8-b9e070f345d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  The striped bats are hanging on their feet for best\n",
            "Tokenized Words:  ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "Stemmed Words:  ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n",
            "Lemmatized Words:  ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ]
    }
  ]
}