{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whitespace Tokenizer: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'lazy', 'dogs', 'in', 'the', 'United', 'States', 'of', 'America.', 'Meanwhile,', 'New', 'York', 'City', 'is', 'bustling', 'with', 'energy', 'and', 'innovation.', 'People', 'from', 'all', 'over', 'the', 'world', 'come', 'to', 'the', 'USA', 'to', 'pursue', 'their', 'dreams', 'and', 'build', 'a', 'better', 'future.']\n",
            "Punctuation Tokenizer: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'lazy', 'dogs', 'in', 'the', 'United', 'States', 'of', 'America', '.', 'Meanwhile', ',', 'New', 'York', 'City', 'is', 'bustling', 'with', 'energy', 'and', 'innovation', '.', 'People', 'from', 'all', 'over', 'the', 'world', 'come', 'to', 'the', 'USA', 'to', 'pursue', 'their', 'dreams', 'and', 'build', 'a', 'better', 'future', '.']\n",
            "Treebank Tokenizer: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'lazy', 'dogs', 'in', 'the', 'United', 'States', 'of', 'America.', 'Meanwhile', ',', 'New', 'York', 'City', 'is', 'bustling', 'with', 'energy', 'and', 'innovation.', 'People', 'from', 'all', 'over', 'the', 'world', 'come', 'to', 'the', 'USA', 'to', 'pursue', 'their', 'dreams', 'and', 'build', 'a', 'better', 'future', '.']\n",
            "Tweet Tokenizer: ['Wow', '!', '!', '!', 'This', 'is', '#awesome', ':)', 'http://example.com', '@user']\n",
            "MWE Tokenizer: ['The', 'quick', 'brown', 'foxes', 'are', 'jumping', 'over', 'lazy', 'dogs', 'in', 'the', 'United_States', 'of', 'America.', 'Meanwhile,', 'New_York', 'City', 'is', 'bustling', 'with', 'energy', 'and', 'innovation.', 'People', 'from', 'all', 'over', 'the', 'world', 'come', 'to', 'the', 'USA', 'to', 'pursue', 'their', 'dreams', 'and', 'build', 'a', 'better', 'future.']\n",
            "Porter Stemmer: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'lazi', 'dog', 'in', 'the', 'unit', 'state', 'of', 'america', '.', 'meanwhil', ',', 'new', 'york', 'citi', 'is', 'bustl', 'with', 'energi', 'and', 'innov', '.', 'peopl', 'from', 'all', 'over', 'the', 'world', 'come', 'to', 'the', 'usa', 'to', 'pursu', 'their', 'dream', 'and', 'build', 'a', 'better', 'futur', '.']\n",
            "Snowball Stemmer: ['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'lazi', 'dog', 'in', 'the', 'unit', 'state', 'of', 'america', '.', 'meanwhil', ',', 'new', 'york', 'citi', 'is', 'bustl', 'with', 'energi', 'and', 'innov', '.', 'peopl', 'from', 'all', 'over', 'the', 'world', 'come', 'to', 'the', 'usa', 'to', 'pursu', 'their', 'dream', 'and', 'build', 'a', 'better', 'futur', '.']\n",
            "Lemmatization: ['The', 'quick', 'brown', 'fox', 'are', 'jumping', 'over', 'lazy', 'dog', 'in', 'the', 'United', 'States', 'of', 'America', '.', 'Meanwhile', ',', 'New', 'York', 'City', 'is', 'bustling', 'with', 'energy', 'and', 'innovation', '.', 'People', 'from', 'all', 'over', 'the', 'world', 'come', 'to', 'the', 'USA', 'to', 'pursue', 'their', 'dream', 'and', 'build', 'a', 'better', 'future', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, TweetTokenizer, MWETokenizer, WhitespaceTokenizer, regexp_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')       # For tokenization\n",
        "nltk.download('wordnet')     # For lemmatization\n",
        "\n",
        "# Bigger sample text for better demonstration\n",
        "text = \"\"\"The quick brown foxes are jumping over lazy dogs in the United States of America. \n",
        "Meanwhile, New York City is bustling with energy and innovation. \n",
        "People from all over the world come to the USA to pursue their dreams and build a better future.\"\"\"\n",
        "\n",
        "# ---------------------- TOKENIZATION SECTION ----------------------\n",
        "\n",
        "# WhitespaceTokenizer simply splits by space\n",
        "print(\"Whitespace Tokenizer:\", WhitespaceTokenizer().tokenize(text))\n",
        "\n",
        "# Regexp tokenizer splits by words and punctuation using regex pattern\n",
        "print(\"Punctuation Tokenizer:\", regexp_tokenize(text, pattern=r\"\\w+|[^\\w\\s]\"))\n",
        "\n",
        "# Treebank tokenizer uses rules to split contractions and punctuation more precisely\n",
        "print(\"Treebank Tokenizer:\", TreebankWordTokenizer().tokenize(text))\n",
        "\n",
        "# TweetTokenizer handles social media text like hashtags, emojis, links\n",
        "tweet_text = \"Wow!!! This is #awesome :) http://example.com @user\"\n",
        "print(\"Tweet Tokenizer:\", TweetTokenizer().tokenize(tweet_text))\n",
        "\n",
        "# MWETokenizer treats multi-word expressions as single tokens (e.g., \"New York\", \"United States\")\n",
        "mwe = MWETokenizer([('New', 'York'), ('United', 'States'), ('United', 'States', 'of', 'America')])\n",
        "print(\"MWE Tokenizer:\", mwe.tokenize(text.split()))\n",
        "\n",
        "# ---------------------- STEMMING SECTION ----------------------\n",
        "\n",
        "# Initialize stemmers\n",
        "ps = PorterStemmer()\n",
        "ss = SnowballStemmer('english')\n",
        "\n",
        "# Tokenize words for stemming\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Apply Porter Stemmer (rule-based stemming)\n",
        "print(\"Porter Stemmer:\", [ps.stem(w) for w in words])\n",
        "\n",
        "# Apply Snowball Stemmer (more refined than Porter)\n",
        "print(\"Snowball Stemmer:\", [ss.stem(w) for w in words])\n",
        "\n",
        "# ---------------------- LEMMATIZATION SECTION ----------------------\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization (returns base dictionary word form)\n",
        "print(\"Lemmatization:\", [lemmatizer.lemmatize(w) for w in words])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
